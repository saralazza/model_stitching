{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f57d9f",
   "metadata": {
    "papermill": {
     "duration": 0.005131,
     "end_time": "2025-11-14T17:32:28.978329",
     "exception": false,
     "start_time": "2025-11-14T17:32:28.973198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa6504",
   "metadata": {
    "papermill": {
     "duration": 0.034517,
     "end_time": "2025-11-14T17:33:04.644660",
     "exception": false,
     "start_time": "2025-11-14T17:33:04.610143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab10f27e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T17:33:04.793314Z",
     "iopub.status.busy": "2025-11-14T17:33:04.793053Z",
     "iopub.status.idle": "2025-11-14T17:33:04.846739Z",
     "shell.execute_reply": "2025-11-14T17:33:04.845578Z"
    },
    "papermill": {
     "duration": 0.090528,
     "end_time": "2025-11-14T17:33:04.848094",
     "exception": false,
     "start_time": "2025-11-14T17:33:04.757566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION (for MLP-Only Model) ---\n",
    "import torch\n",
    "\n",
    "CONFIG = {\n",
    "    # --- MODIFIED: Single model path template ---\n",
    "    \"MODEL_PATH_TPL\": \"best_translator_mlp_fold_{}.pth\",\n",
    "    \"DEVICE\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \n",
    "    # --- Training ---\n",
    "    \"EPOCHS\": 100, # Your log shows 100\n",
    "    \"PATIENCE\": 25,\n",
    "    \n",
    "    # --- K-Fold Configuration ---\n",
    "    \"N_FOLDS\": 10,  # Your log shows 5\n",
    "    \"RANDOM_STATE\": 42,\n",
    "    \n",
    "    # --- Architecture/Loss Params (Using your tuned values) ---\n",
    "    \"HIDDEN_FEATURES\": 3072, \n",
    "    \"NUM_BLOCKS\": 2,\n",
    "    \"DROPOUT_RATE\": 0.3832296399028748,     \n",
    "    \"LR\": 1.7749224152998916e-05,             \n",
    "    \"WEIGHT_DECAY\": 0.0008723639231133962,\n",
    "    \n",
    "    # --- Hybrid Loss Params ---\n",
    "    \"MARGIN\": 0.3898481154806984,\n",
    "    \"TEMPERATURE\": 0.010879304099273002,\n",
    "    \"HYBRID_ALPHA\": 0.0, # Kept at 0\n",
    "    \"MIXUP_ALPHA\": 0.23197266859653176,\n",
    "    \"LABEL_SMOOTHING\": 0.12,\n",
    "    \n",
    "    # --- General Settings ---\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"ACCUMULATION_STEPS\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97fbd70f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T17:33:04.927396Z",
     "iopub.status.busy": "2025-11-14T17:33:04.926605Z",
     "iopub.status.idle": "2025-11-14T17:33:13.799270Z",
     "shell.execute_reply": "2025-11-14T17:33:13.798504Z"
    },
    "papermill": {
     "duration": 8.912128,
     "end_time": "2025-11-14T17:33:13.800409",
     "exception": false,
     "start_time": "2025-11-14T17:33:04.888281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clean indices from '/kaggle/input/clean-dataset/'...\n",
      "Loaded 123460 clean caption indices.\n",
      "Filtering embeddings based on clean indices...\n",
      "Clean data shape (text): torch.Size([123460, 1024])\n",
      "Clean data shape (image): torch.Size([123460, 1536])\n",
      "\n",
      "CLEAN Data loaded: 123460 text/image pairs.\n",
      "KFold splitter created for 123460 pairs.\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Data Loading (MODIFIED for Cleaned Data) ---\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    # --- 1. Load Full (Noisy) Data ---\n",
    "    full_train_data = np.load(\"/kaggle/input/aml-challenge-dataset/train.npz\", allow_pickle=True)\n",
    "    all_text_embeddings_full = torch.from_numpy(full_train_data['captions/embeddings']).float()\n",
    "    all_image_embeddings_full = torch.from_numpy(full_train_data['images/embeddings']).float()\n",
    "    all_image_ids_full = full_train_data['images/names']\n",
    "    \n",
    "    num_images_total = all_image_embeddings_full.shape[0]\n",
    "    num_captions_per_image = all_text_embeddings_full.shape[0] // num_images_total\n",
    "\n",
    "    # --- 2. Load the Clean Indices ---\n",
    "    print(\"Loading clean indices from '/kaggle/input/clean-dataset/'...\")\n",
    "    clean_caption_indices = np.load(\"/kaggle/input/clean-dataset/clean_caption_indices.npy\")\n",
    "    # clean_image_indices = np.load(\"/kaggle/input/clean-dataset/clean_image_indices.npy\")\n",
    "    print(f\"Loaded {len(clean_caption_indices)} clean caption indices.\")\n",
    "    \n",
    "    # --- 3. Filter Embeddings ---\n",
    "    # We will build our new dataset from the clean CAPTION indices\n",
    "    # This creates a 1-to-1 (text, image) dataset\n",
    "    \n",
    "    print(\"Filtering embeddings based on clean indices...\")\n",
    "    # Filter text embeddings (e.g., 100,000 x 1024)\n",
    "    all_text_embeddings = all_text_embeddings_full[clean_caption_indices]\n",
    "    \n",
    "    # Find the corresponding image for each clean caption\n",
    "    # e.g., caption 12 -> image 2 (12 // 5)\n",
    "    # e.g., caption 14 -> image 2 (14 // 5)\n",
    "    corresponding_image_indices = clean_caption_indices // num_captions_per_image\n",
    "    \n",
    "    # Filter image embeddings (e.g., 100,000 x 1536)\n",
    "    all_image_embeddings = all_image_embeddings_full[corresponding_image_indices]\n",
    "    \n",
    "    print(f\"Clean data shape (text): {all_text_embeddings.shape}\")\n",
    "    print(f\"Clean data shape (image): {all_image_embeddings.shape}\")\n",
    "\n",
    "    # --- 4. Setup K-Fold for 1-to-1 Pairs ---\n",
    "    # We now split the PAIRS, not the images\n",
    "    num_clean_pairs = len(all_text_embeddings)\n",
    "    pair_indices = np.arange(num_clean_pairs)\n",
    "    \n",
    "    kfold = KFold(n_splits=CONFIG['N_FOLDS'], shuffle=True, random_state=CONFIG['RANDOM_STATE'])\n",
    "\n",
    "    print(f\"\\nCLEAN Data loaded: {num_clean_pairs} text/image pairs.\")\n",
    "    print(f\"KFold splitter created for {num_clean_pairs} pairs.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure 'train.npz' is at '/kaggle/input/train/train/train.npz'\")\n",
    "    print(\"And your 'clean-dataset' is attached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07e2a3",
   "metadata": {
    "papermill": {
     "duration": 0.035853,
     "end_time": "2025-11-14T17:33:14.238489",
     "exception": false,
     "start_time": "2025-11-14T17:33:14.202636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d2f5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T17:33:14.313381Z",
     "iopub.status.busy": "2025-11-14T17:33:14.313113Z",
     "iopub.status.idle": "2025-11-14T17:33:14.321174Z",
     "shell.execute_reply": "2025-11-14T17:33:14.320469Z"
    },
    "papermill": {
     "duration": 0.04681,
     "end_time": "2025-11-14T17:33:14.322412",
     "exception": false,
     "start_time": "2025-11-14T17:33:14.275602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Model Architecture (MODIFIED: MLP-Only) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 3A. The GeGLU Activation Module (Unchanged) ---\n",
    "class GeGLU(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_features, out_features * 2)\n",
    "        self.gelu = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        proj_out = self.proj(x)\n",
    "        a, b = proj_out.chunk(2, dim=-1)\n",
    "        return self.gelu(a) * b\n",
    "\n",
    "# --- 3B. The Residual Block with GeGLU (Unchanged) ---\n",
    "class ResidualBlockGeGLU(nn.Module):\n",
    "    def __init__(self, features, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            GeGLU(features, features), \n",
    "            nn.BatchNorm1d(features),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# --- 3C. REMOVED TextVariationalEncoder ---\n",
    "# This class is no longer needed for this architecture.\n",
    "\n",
    "# --- 3D. The Translator (Your HybridMLP) (Unchanged) ---\n",
    "# This class is already flexible. \n",
    "# 'in_features' will be 1024 (text dim) when we create the model.\n",
    "class TranslatorMLP(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, num_blocks, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        backbone_layers = [\n",
    "            nn.Linear(in_features, hidden_features, bias=False),\n",
    "            nn.BatchNorm1d(hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5)\n",
    "        ]\n",
    "        for _ in range(num_blocks):\n",
    "            backbone_layers.append(\n",
    "                ResidualBlockGeGLU(hidden_features, dropout_rate) # Using GeGLU\n",
    "            )\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "        \n",
    "        self.translator_head = nn.Sequential(\n",
    "            nn.Linear(hidden_features, hidden_features // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_features // 2, out_features) # out_features = 1536\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_representation = self.backbone(x)\n",
    "        output = self.translator_head(shared_representation)\n",
    "        # We keep the normalization, as discussed\n",
    "        return F.normalize(output, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b28919",
   "metadata": {
    "papermill": {
     "duration": 0.034085,
     "end_time": "2025-11-14T17:33:14.601656",
     "exception": false,
     "start_time": "2025-11-14T17:33:14.567571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "961dbff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T17:33:14.672253Z",
     "iopub.status.busy": "2025-11-14T17:33:14.671948Z",
     "iopub.status.idle": "2025-11-14T17:33:14.683648Z",
     "shell.execute_reply": "2025-11-14T17:33:14.683075Z"
    },
    "papermill": {
     "duration": 0.049207,
     "end_time": "2025-11-14T17:33:14.684810",
     "exception": false,
     "start_time": "2025-11-14T17:33:14.635603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 5. Helper Functions (for MLP-Only Model) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- 5A. Loss Functions (Unchanged) ---\n",
    "class PureContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, label_smoothing=0.12):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.label_smoothing = label_smoothing # <-- MODIFIED\n",
    "    def forward(self, pred_norm, target_norm):\n",
    "        sim_matrix = torch.matmul(pred_norm, target_norm.T) / self.temperature\n",
    "        labels = torch.arange(pred_norm.size(0), device=pred_norm.device)\n",
    "        return F.cross_entropy(sim_matrix, labels, label_smoothing=self.label_smoothing)\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, text_embeds, img_embeds_norm):\n",
    "        sim_matrix = text_embeds @ img_embeds_norm.T\n",
    "        positive_scores = torch.diag(sim_matrix)\n",
    "        mask = torch.eye(text_embeds.size(0), dtype=torch.bool, device=text_embeds.device)\n",
    "        sim_matrix_masked = sim_matrix.masked_fill(mask, -float('inf'))\n",
    "        hard_negative_scores = sim_matrix_masked.max(dim=1)[0]\n",
    "        return F.relu(self.margin - positive_scores + hard_negative_scores).mean()\n",
    "\n",
    "# --- 5B. FAST VALIDATION (MODIFIED for single model) ---\n",
    "# This version only takes 3 arguments\n",
    "def validation_fn(translator_model, val_loader, criterion_triplet, criterion_pure, alpha, device):\n",
    "    translator_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, image_batch in val_loader:\n",
    "            text_batch, image_batch = text_batch.to(device), image_batch.to(device)\n",
    "            \n",
    "            pred_embeddings = translator_model(text_batch)\n",
    "            target_embeddings = F.normalize(image_batch, p=2, dim=1)\n",
    "            \n",
    "            loss_triplet = criterion_triplet(pred_embeddings, target_embeddings)\n",
    "            loss_pure = criterion_pure(pred_embeddings, target_embeddings)\n",
    "            hybrid_loss = (alpha * loss_triplet) + ((1 - alpha) * loss_pure)\n",
    "            \n",
    "            val_loss += hybrid_loss.item()\n",
    "            \n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# --- 5C. MRR CALCULATION (MODIFIED for single model) ---\n",
    "# This version only takes 3 arguments\n",
    "def calculate_mrr(translator_model, val_loader, device):\n",
    "    translator_model.eval()\n",
    "    all_reciprocal_ranks = []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, image_batch in val_loader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            pred_embeddings_norm = translator_model(text_batch)\n",
    "            target_embeddings_norm = F.normalize(image_batch, p=2, dim=1)\n",
    "            sim_matrix = pred_embeddings_norm @ target_embeddings_norm.T\n",
    "            \n",
    "            correct_scores = torch.diag(sim_matrix).unsqueeze(1)\n",
    "            ranks = (sim_matrix >= correct_scores).sum(dim=1).float()\n",
    "            reciprocal_ranks = 1.0 / ranks\n",
    "            all_reciprocal_ranks.append(reciprocal_ranks)\n",
    "            \n",
    "    mrr = torch.cat(all_reciprocal_ranks).mean().item()\n",
    "    return mrr\n",
    "\n",
    "# --- 5D. MIXUP (Unchanged) ---\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0: lam = np.random.beta(alpha, alpha)\n",
    "    else: lam = 1\n",
    "    batch_size = x.size(0)\n",
    "    indices = torch.randperm(batch_size, device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[indices, :]\n",
    "    mixed_y = lam * y + (1 - lam) * y[indices, :]\n",
    "    return mixed_x, mixed_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0200962",
   "metadata": {
    "papermill": {
     "duration": 0.032944,
     "end_time": "2025-11-14T17:33:14.752048",
     "exception": false,
     "start_time": "2025-11-14T17:33:14.719104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 16. FINAL Main Training Script (MODIFIED for 1-to-1 Clean Data) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --- Store MRR for each fold ---\n",
    "all_fold_mrr = []\n",
    "\n",
    "# =====================================================================================\n",
    "#  K-FOLD OUTER LOOP (Simplified for 1-to-1 data)\n",
    "# =====================================================================================\n",
    "# kfold and pair_indices come from Cell 10\n",
    "for fold, (train_pair_indices, val_pair_indices) in enumerate(kfold.split(pair_indices)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"--- STARTING FOLD {fold+1}/{CONFIG['N_FOLDS']} ---\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # --- 1. Create Datasets & DataLoaders for this fold ---\n",
    "    \n",
    "    # Simple indexing using the pair indices\n",
    "    train_text_emb = all_text_embeddings[train_pair_indices]\n",
    "    train_image_emb = all_image_embeddings[train_pair_indices]\n",
    "    \n",
    "    val_text_emb = all_text_embeddings[val_pair_indices]\n",
    "    val_image_emb = all_image_embeddings[val_pair_indices]\n",
    "\n",
    "    # Create simple 1-to-1 TensorDatasets\n",
    "    train_dataset = TensorDataset(train_text_emb, train_image_emb)\n",
    "    val_dataset = TensorDataset(val_text_emb, val_image_emb)\n",
    "\n",
    "    # --- Use the seeded DataLoaders ---\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['BATCH_SIZE'], \n",
    "        shuffle=True,\n",
    "        worker_init_fn=seed_worker, \n",
    "        generator=g                 \n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'])\n",
    "\n",
    "    print(f\"Fold {fold+1}: Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
    "\n",
    "    # (Optional) We don't need to save split info, as it's just indices\n",
    "\n",
    "    # --- 2. Re-initialize Models, Losses, and Optimizer ---\n",
    "    translator_model = TranslatorMLP(\n",
    "        in_features=all_text_embeddings.shape[1], \n",
    "        out_features=all_image_embeddings.shape[1], \n",
    "        hidden_features=CONFIG['HIDDEN_FEATURES'],\n",
    "        num_blocks=CONFIG['NUM_BLOCKS'],\n",
    "        dropout_rate=CONFIG['DROPOUT_RATE']\n",
    "    ).to(CONFIG['DEVICE'])\n",
    "\n",
    "    criterion_triplet = TripletLoss(margin=CONFIG['MARGIN'])\n",
    "    criterion_pure = PureContrastiveLoss(\n",
    "        temperature=CONFIG['TEMPERATURE'],\n",
    "        label_smoothing=CONFIG['LABEL_SMOOTHING'] \n",
    "    )\n",
    "\n",
    "    # --- Use Optimizer from CONFIG (e.g., AdamW) ---\n",
    "    optimizer = optim.AdamW(\n",
    "        translator_model.parameters(),\n",
    "        lr=CONFIG['LR'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * CONFIG['EPOCHS'] // CONFIG['ACCUMULATION_STEPS']\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "    \n",
    "    # --- 3. Training Loop for this fold ---\n",
    "    best_finetune_mrr = -1.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(CONFIG['EPOCHS']):\n",
    "        translator_model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        batch_iterator = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{CONFIG['EPOCHS']}\")\n",
    "        \n",
    "        # --- MODIFIED: Loop is now simpler (text, image) ---\n",
    "        for i, (text_batch, image_batch) in enumerate(batch_iterator):\n",
    "            text_batch = text_batch.to(CONFIG['DEVICE'])\n",
    "            image_batch = image_batch.to(CONFIG['DEVICE'])\n",
    "            \n",
    "            target_embeddings = F.normalize(image_batch, p=2, dim=1)\n",
    "            mixed_text, mixed_targets = mixup_data(text_batch, target_embeddings, alpha=CONFIG['MIXUP_ALPHA'])\n",
    "\n",
    "            pred_embeddings = translator_model(mixed_text)\n",
    "            \n",
    "            loss_triplet = criterion_triplet(pred_embeddings, mixed_targets)\n",
    "            loss_pure = criterion_pure(pred_embeddings, mixed_targets)\n",
    "            loss = (CONFIG['HYBRID_ALPHA'] * loss_triplet) + ((1 - CONFIG['HYBRID_ALPHA']) * loss_pure)\n",
    "            \n",
    "            loss = loss / CONFIG['ACCUMULATION_STEPS']\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % CONFIG['ACCUMULATION_STEPS'] == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_train_loss += loss.item() * CONFIG['ACCUMULATION_STEPS']\n",
    "            batch_iterator.set_postfix({\"Train Loss\": f\"{loss.item() * CONFIG['ACCUMULATION_STEPS']:.4f}\"})\n",
    "            \n",
    "        # --- Validation ---\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        val_loss = validation_fn(\n",
    "            translator_model, \n",
    "            val_loader, \n",
    "            criterion_triplet, \n",
    "            criterion_pure, \n",
    "            CONFIG['HYBRID_ALPHA'], \n",
    "            CONFIG['DEVICE']\n",
    "        )\n",
    "        val_mrr = calculate_mrr(translator_model, val_loader, CONFIG['DEVICE'])\n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Fold {fold+1} Epoch {epoch+1} -> Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MRR: {val_mrr:.4f} | LR: {current_lr:e}\")\n",
    "        \n",
    "        if val_mrr > best_finetune_mrr:\n",
    "            best_finetune_mrr = val_mrr\n",
    "            \n",
    "            model_path = CONFIG['MODEL_PATH_TPL'].format(fold)\n",
    "            torch.save(translator_model.state_dict(), model_path)\n",
    "            print(f\"  ðŸ† New best MRR for Fold {fold+1}! Saving model to {model_path}...\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG['PATIENCE']:\n",
    "                print(f\"Early stopping triggered for Fold {fold+1}.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nâœ… Fold {fold+1} finished. Best validation MRR: {best_finetune_mrr:.4f}\")\n",
    "    all_fold_mrr.append(best_finetune_mrr)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"âœ… All {CONFIG['N_FOLDS']} folds trained! \")\n",
    "print(f\"Mean Validation MRR: {np.mean(all_fold_mrr):.4f} (Std: {np.std(all_fold_mrr):.4f})\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fdac05",
   "metadata": {
    "papermill": {
     "duration": 22.564847,
     "end_time": "2025-11-14T20:16:58.435378",
     "exception": false,
     "start_time": "2025-11-14T20:16:35.870531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b136f",
   "metadata": {
    "papermill": {
     "duration": 22.765772,
     "end_time": "2025-11-14T20:17:43.123281",
     "exception": false,
     "start_time": "2025-11-14T20:17:20.357509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb34bf8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:19:13.685519Z",
     "iopub.status.busy": "2025-11-14T20:19:13.685246Z",
     "iopub.status.idle": "2025-11-14T20:19:15.161451Z",
     "shell.execute_reply": "2025-11-14T20:19:15.160466Z"
    },
    "papermill": {
     "duration": 24.02017,
     "end_time": "2025-11-14T20:19:15.162625",
     "exception": true,
     "start_time": "2025-11-14T20:18:51.142455",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating submission.csv using K-Fold MLP models (Averaging) ---\n",
      "ERROR: Please make sure the 'test.clean.npz' file is in the correct directory.\n",
      "\n",
      "--- Generating predictions for Fold 1/10 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/4286197566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mfold_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtext_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Predicting Fold {fold+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEVICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 8. GENERATE FINAL SUBMISSION FILE (for MLP-Only) ---\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"--- Generating submission.csv using K-Fold MLP models (Averaging) ---\")\n",
    "\n",
    "# --- 1. Load the test data ---\n",
    "try:\n",
    "    test_data = np.load(\"/kaggle/input/test.clean.npz\", allow_pickle=True)\n",
    "    test_text_emb = torch.from_numpy(test_data['captions/embeddings']).float()\n",
    "    test_caption_ids = test_data['captions/ids']\n",
    "    test_loader = DataLoader(test_text_emb, batch_size=CONFIG['BATCH_SIZE'])\n",
    "    print(f\"Test data and caption IDs loaded: {len(test_caption_ids)} samples.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Please make sure the 'test.clean.npz' file is in the correct directory.\")\n",
    "\n",
    "# --- 2. Loop through all folds to get predictions ---\n",
    "all_predictions_list = []\n",
    "\n",
    "for fold in range(CONFIG['N_FOLDS']):\n",
    "    print(f\"\\n--- Generating predictions for Fold {fold+1}/{CONFIG['N_FOLDS']} ---\")\n",
    "    \n",
    "    # --- Define model path for this fold ---\n",
    "    model_path = CONFIG['MODEL_PATH_TPL'].format(fold)\n",
    "\n",
    "    # --- MODIFIED: Load only the TranslatorMLP ---\n",
    "    translator_model = TranslatorMLP(\n",
    "        in_features=all_text_embeddings.shape[1], # 1024\n",
    "        out_features=all_image_embeddings.shape[1], # 1536\n",
    "        hidden_features=CONFIG['HIDDEN_FEATURES'],\n",
    "        num_blocks=CONFIG['NUM_BLOCKS'],\n",
    "        dropout_rate=CONFIG['DROPOUT_RATE']\n",
    "    ).to(CONFIG['DEVICE'])\n",
    "\n",
    "    try:\n",
    "        translator_model.load_state_dict(torch.load(model_path))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Could not find model file for fold {fold}: {model_path}\")\n",
    "        continue \n",
    "        \n",
    "    translator_model.eval()\n",
    "\n",
    "    # --- Create predictions in batches for this fold ---\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text_batch in tqdm(test_loader, desc=f\"Predicting Fold {fold+1}\"):\n",
    "            text_batch = text_batch.to(CONFIG['DEVICE'])\n",
    "            \n",
    "            # --- MODIFIED: Direct MLP flow ---\n",
    "            pred_batch = translator_model(text_batch)\n",
    "            \n",
    "            fold_predictions.append(pred_batch.cpu().numpy())\n",
    "    \n",
    "    all_predictions_list.append(np.concatenate(fold_predictions, axis=0))\n",
    "\n",
    "if not all_predictions_list:\n",
    "    print(\"\\nNo predictions were generated. Cannot create submission file.\")\n",
    "else:\n",
    "    # --- 3. Average the predictions from all folds ---\n",
    "    print(f\"\\n--- Averaging predictions from {len(all_predictions_list)} folds ---\")\n",
    "    avg_predictions = np.mean(all_predictions_list, axis=0)\n",
    "    \n",
    "    # --- 4. Format for CSV submission ---\n",
    "    embedding_json_list = [json.dumps(embedding.tolist()) for embedding in avg_predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_caption_ids,\n",
    "        'embedding': embedding_json_list\n",
    "    })\n",
    "\n",
    "    # --- 5. Save the DataFrame to a CSV file ---\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nâœ… Ensembled submission file 'submission.csv' has been generated successfully!\")\n",
    "    print(\"Here's a preview of the first 5 rows:\")\n",
    "    print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8587732,
     "sourceId": 13524858,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8737799,
     "sourceId": 13733245,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10034.370709,
   "end_time": "2025-11-14T20:19:39.769896",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-14T17:32:25.399187",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
